# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kJaOif9_1nDWqdLgMNgRSJPk_Sogs2-h
"""

"""Final_code:"""

#nltk load:
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

#data preprocessing
def preprocess(filename_train,filename_test):
  from nltk.tokenize import sent_tokenize, word_tokenize
  from nltk.stem import PorterStemmer
  from nltk.tokenize import RegexpTokenizer
  from nltk.stem import WordNetLemmatizer 
  from nltk.corpus import wordnet
  from nltk.corpus import stopwords 
  from nltk.tokenize import word_tokenize
  import json 
  lemmatizer = WordNetLemmatizer() 
  porter = PorterStemmer()
  def lemma(sentence):
    lemma_sen = []
    word_list = nltk.word_tokenize(sentence)
    lemma_sen = ' '.join([lemmatizer.lemmatize(w.lower(),pos='v') for w in word_list])
    return lemma_sen
  def stemSentence(sentence):
    stem_sentence=[]
    token_words=word_tokenize(str(sentence))
    for word in token_words:
      stem_sentence.append(porter.stem(word))
      stem_sentence.append(" ")
    return stem_sentence
  def rmpunctuation(sentence):
    senapp = []
    for i in sentence:
      tokenizer = RegexpTokenizer(r'\w+')
      rmsentence = tokenizer.tokenize(str(i))
      senapp.append(" ".join(rmsentence))
    return senapp
  def rmstwords(sentence):   
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(sentence.lower())
    fil_sen = [w for w in word_tokens if not w in stop_words]
    return fil_sen
  def dataset(filename):
    data = [json.loads(line) for line in open(filename, 'r')]
    labels = []
    sentence1 = []
    sentence2 = []
    count = 0
    for i in data:
      if i['gold_label'] != '-':
        sentence1.append(i['sentence1'])
        sentence2.append(i['sentence2'])
        labels.append(i['gold_label'])      
    sentence1 = np.array(sentence1)
    sentence2 = np.array(sentence2)
    labels = np.array(labels)
    s1_rmst = [rmstwords(str(i)) for i in sentence1]
    s1_lemma = [lemma(str(i)) for i in s1_rmst]
    s1_final = rmpunctuation(s1_lemma)
    s2_rmst = [rmstwords(str(i)) for i in sentence2]
    s2_stem = [lemma(str(i)) for i in s2_rmst]
    s2_final = rmpunctuation(s2_stem) 
    return s1_final,s2_final,labels
  sen1_train,sen2_train,Ytrain = dataset(filename_train)
  sen1_test,sen2_test,Ytest = dataset(filename_test)
  #print(labels)
  from sklearn.preprocessing import LabelEncoder
  import tensorflow as tf
  encoder = LabelEncoder()
  encoder.fit(Ytrain)
  encoded_Y = encoder.transform(Ytrain)
  Ytrain_encode = tf.keras.utils.to_categorical(encoded_Y)
  encoded_Y = encoder.transform(Ytest)
  Ytest_encode = tf.keras.utils.to_categorical(encoded_Y)
  return sen1_train,sen2_train,Ytrain_encode,sen1_test,sen2_test,Ytest_encode

import numpy as np
import json
import pandas as pd
import pickle
from numpy import array
from numpy import asarray
from numpy import zeros
from tensorflow import keras
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Masking

#tokenize and word embedding
def embedding(sen1_train,sen2_train,sen1_test,sen2_test):
  
  t = Tokenizer()
  t.fit_on_texts(sen1_train)
  t.fit_on_texts(sen2_train)
  vocab_size = len(t.word_index) + 1
  vocab_size1 = len(t.word_index) + 1
  encoded_docs = t.texts_to_sequences(sen1_train)
  encoded_docs1 = t.texts_to_sequences(sen2_train)
  max_length = 56
  padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
  padded_docs1 = pad_sequences(encoded_docs1, maxlen=max_length, padding='post')
  p1 = np.array(padded_docs)
  p2 = np.array(padded_docs1)
  import pickle
  with open('embedding.pickle', 'rb') as handle:
    embedding_matrix = pickle.load(handle)
  encoded_docst = t.texts_to_sequences(sen1_test)
  encoded_docst1 = t.texts_to_sequences(sen2_test)
  padded_docst = pad_sequences(encoded_docst, maxlen=max_length, padding='post')
  padded_docst1 = pad_sequences(encoded_docst1, maxlen=max_length, padding='post')
  pt1 = np.array(padded_docst)
  pt2 = np.array(padded_docst1)
  return p1,p2,pt1,pt2,vocab_size,embedding_matrix

#model
def create_model(vocab_size,embedding_matrix): 

  input1 =Input(shape = (56))
  e1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=56, trainable=False, mask_zero = True)(input1)
  #m1 = Masking(mask_value=0)(e1)
  LSTM_IP_layer1 = Bidirectional(LSTM(300, input_shape=(56,300)))(e1)
  OP1 = Dropout(0.25)(LSTM_IP_layer1)
  #Pool1 = AveragePooling3D(pool_size = 2, padding = 'valid')(OP1)

  input2 = Input(shape = (56))
  e2 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=56, trainable=False, mask_zero = True)(input2)
  #m2 = Masking(mask_value=0.0)(e2)
  LSTM_IP_layer2 = Bidirectional(LSTM(300, input_shape=(56,300)))(e2)
  OP2 = Dropout(0.25)(LSTM_IP_layer2)
  #Pool2 = AveragePooling3D(pool_size = 2, padding = 'valid')(OP2)

  def Func(x):
      return x[0] * x[1]
  Lambda_1 = tf.keras.layers.Lambda(Func, name = 'Lambda_1')([OP1,OP2])

  def Func1(x):
      return x[0] - x[1]
  Lambda_2 = tf.keras.layers.Lambda(Func1, name = 'Lambda_2')([OP1,OP2])

  #concatl = tf.keras.layers.Concatenate(axis=1)([OP1,OP2])
  concat = tf.keras.layers.Concatenate(axis=1)([Lambda_1, Lambda_2])
  flat = Flatten()(concat)
  Dense_L1 = Dense(400,activation='relu')(flat)
  Drop = Dropout(0.1)(Dense_L1)
  Dense_L2 = Dense(400,activation='relu')(Drop)
  Drop1 = Dropout(0.1)(Dense_L2)
  output = tf.keras.layers.Dense(3,activation='softmax')(Drop1)
  full_model = tf.keras.Model(inputs = [input1,input2], outputs=output)
  full_model.compile(optimizer = 'rmsprop', loss='categorical_crossentropy', metrics = ['accuracy'])
  return full_model

from sklearn import metrics
if __name__ == "__main__":
  sen1_train,sen2_train,Ytrain,sen1_test,sen2_test,Ytest = preprocess('drive/My Drive/SNLI/snli_1.0_train.jsonl','drive/My Drive/SNLI/snli_1.0_test.jsonl')
  Xtrain1,Xtrain2,Xtest1,Xtest2,vocab_size,embedding_matrix = embedding(sen1_train,sen2_train,sen1_test,sen2_test)
  model = create_model(vocab_size,embedding_matrix)
  history = model.fit([Xtrain1,Xtrain2],Ytrain,batch_size=500,validation_split = 0.2,epochs=8)
  model.save('models/Deep_Learning_model12.h5')
  
