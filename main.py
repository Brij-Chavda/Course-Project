# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z-8gghoT8i0teC-ZHHY0H9I3ULWlVQcR
"""

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import wordnet
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
lemmatizer = WordNetLemmatizer() 
porter = PorterStemmer()
def lemma(sentence):
    lemma_sen = []
    word_list = nltk.word_tokenize(sentence)
    lemma_sen = ' '.join([lemmatizer.lemmatize(w.lower(),pos='v') for w in word_list])
    return lemma_sen
def stemSentence(sentence):
    stem_sentence=[]
    token_words=word_tokenize(str(sentence))
    for word in token_words:
        stem_sentence.append(porter.stem(word))
        stem_sentence.append(" ")
    return stem_sentence
def rmpunctuation(sentence):
    senapp = []
    for i in sentence:
        tokenizer = RegexpTokenizer(r'\w+')
        rmsentence = tokenizer.tokenize(str(i))
        senapp.append(" ".join(rmsentence))
    return senapp
def rmstwords(sentence):   
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(sentence.lower())
    fil_sen = [w for w in word_tokens if not w in stop_words]
    return fil_sen

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

import json
test_data = [json.loads(line) for line in open('snli_1.0_test.jsonl', 'r')]

test_labels = []
test_sentence1 = []
test_sentence2 = []
for i in test_data:
    test_sentence1.append(i['sentence1'])
    test_sentence2.append(i['sentence2'])
    test_labels.append(i['gold_label'])

sen1_rmst = [rmstwords(str(i)) for i in test_sentence1]
sen1_lemma = [lemma(str(i)) for i in sen1_rmst]
sen1_final_test = rmpunctuation(sen1_lemma)
sen1_lemma = []
sen1_rmst = []
sen2_rmst = [rmstwords(str(i)) for i in test_sentence2]
sen2_lemma = [lemma(str(i)) for i in sen2_rmst]
sen2_final_test = rmpunctuation(sen2_lemma) 
sen2_lemma = []
sen2_rmst = []

"""# TFIDF Model load:#"""

import pickle
load_tfidf_model = pickle.load(open('models/logistic_model.sav', 'rb'))

tfsen1_vec = pickle.load(open("tansformer.pickle", 'rb'))
tfsen2_vec = pickle.load(open("tansformer1.pickle", 'rb'))

tf_sen1 = tfsen1_vec.transform(sen1_final_test)
tf_sen2 = tfsen2_vec.transform(sen2_final_test)

from scipy.sparse import hstack
X = hstack([tf_sen1,tf_sen2])
predict_data_tf = load_tfidf_model.predict(X)

predict_label = []
for i in predict_data_tf:
    if i == 0:
        predict_label.append('neutral')
    elif i == 1:
        predict_label.append('contradiction')
    else:
        predict_label.append('entailment')

n_true = 0
n_total = 0
for i in range(len(predict_label)):
  if test_labels[i] == predict_label[i]:
    n_true = n_true + 1
    n_total = n_total + 1 
  elif test_labels[i] != '-':
    n_total = n_total + 1 
print("tf-idf model accuracy is %f" % (n_true/n_total))

file = open("tfidf.txt",'w',encoding='utf-8')
for i in range(len(predict_label)):
    file.write(str(predict_label[i]))
    file.write('\n')
file.close()

"""# Deep Learning model"""

#load model
import tensorflow as tf
from tensorflow import keras 

model = tf.keras.models.load_model('models/Deep_learning_model.h5')

#tokenizer preprocessing
import pickle
with open('tokenizer.pickle','rb') as handle:
  t = pickle.load(handle)
with open('encoderbin','rb') as f:
  encoder = pickle.load(f)

#label encoding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
encoded1 = t.texts_to_sequences(sen1_final_test)
encoded2 = t.texts_to_sequences(sen2_final_test)
pad1 = pad_sequences(encoded1,maxlen = 56, padding = 'post')
pad2 = pad_sequences(encoded2,maxlen = 56, padding = 'post')

#accuracy   #label decoding
predict_data_deep = model.predict([pad1,pad2])
class_data = np.argmax(predict_data_deep,1)
entailment_class = encoder.inverse_transform(class_data)
n_true = 0
n_total = 0
for i in range(len(entailment_class)):
  if test_labels[i] == entailment_class[i]:
    n_true = n_true + 1
    n_total = n_total + 1 
  elif test_labels[i] != '-':
    n_total = n_total + 1 
print("Deep learning model accuracy is %f" % (n_true/n_total))

#file writing
predict_labeld = encoder.inverse_transform(class_data)
file = open("deep_model.txt",'w',encoding='utf-8')
for i in range(len(predict_labeld)):
    file.write(str(predict_labeld[i]))
    file.write('\n')
file.close()

